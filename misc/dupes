#!/usr/bin/env python
# Released under WTFPL v2 <http://sam.zoy.org/wtfpl/>

import os
import sys
import stat
import hashlib
from collections import defaultdict
from optparse import OptionParser

# this doesn't need to be declared here
# I'm just doing so as a reminder that `opts` is global
opts = None
_ttywidth = None

# header and hash caches, to avoid reading
# or hashing the same file twice
header_size = 512
file_headers = {}   # path → header
file_hashes = {}    # path → hash

def ttywidth():
    global _ttywidth
    if _ttywidth is None:
        with os.popen("stty size", "r") as fh:
            line = fh.read().strip()
        rows, cols = line.split()
        _ttywidth = int(cols)
    return _ttywidth

def status(*args):
    if sys.stderr.isatty():
        msg = " ".join(args)
        msg = msg[:ttywidth()]
        sys.stderr.write("\r\033[K\033[33m%s\033[m" % msg)
        sys.stderr.flush()

def weed_ignores(dirs):
    ignores = {".git", ".hg"}
    for item in dirs[:]:
        if item in ignores:
            dirs.remove(item)

def enum_files(root_dir):
    for subdir, dirs, files in os.walk(root_dir):
        weed_ignores(dirs)
        for name in files:
            path = os.path.join(subdir, name)
            yield path

def get_header(path):
    if path not in file_headers:
        if opts.verbose:
            print("reading", path)
        with open(path, "rb") as fh:
            file_headers[path] = fh.read(header_size)
    return file_headers[path]

def hash_file(path):
    if path not in file_hashes:
        if opts.verbose:
            print("hashing", path)
        h = hashlib.sha1()
        with open(path, "rb") as fh:
            buf = True
            while buf:
                buf = fh.read(4194304)
                h.update(buf)
        file_hashes[path] = h.digest()
    return file_hashes[path]

def find_duplicates(root_dirs):
    # dicts keeping duplicate items
    known_sizes = defaultdict(list)     # size → path[]
    known_headers = defaultdict(list)   # (size, header) → path[]
    known_hashes = defaultdict(list)    # (size, hash) → path[]

    # find files identical in size
    for root_dir in root_dirs:
        for path in enum_files(root_dir):
            status("stat", path)
            st = os.lstat(path)
            if not stat.S_ISREG(st.st_mode):
                continue
            known_sizes[st.st_size].append(path)

    status()

    # find files identical in size and first `header_size` bytes
    for size, paths in known_sizes.items():
        if len(paths) < 2:
            continue

        for path in paths:
            status("head", path)
            header = get_header(path)
            known_headers[size, header].append(path)

    status()

    # find files identical in size and hash
    for (size, header), paths in known_headers.items():
        if len(paths) < 2:
            continue
        if size <= header_size:
            # optimization: don't compare by hash if
            # the entire contents are already known
            status()
            yield paths
            continue

        for path in paths:
            status("hash", path)
            filehash = hash_file(path)
            known_hashes[size, filehash].append(path)

    status()

    for (size, filehash), paths in known_hashes.items():
        if len(paths) < 2:
            continue
        yield paths

cli_usage = "%prog [options] {path}"

cli_desc = """\
Finds files with duplicate data.
"""

cli_epilog = """\
This program ignores symlinks, special files, and the like. It also does not know about hardlinks; this might be added as an optimization later.
"""

cli_version = "1.∞"

if __name__ == "__main__":
    op = OptionParser(
        usage=cli_usage,
        description=cli_desc,
        epilog=cli_epilog,
        version=cli_version)
    op.add_option("-v", "--verbose", dest="verbose", action="store_true", default=False,
        help="show files as they are processed")

    opts, args = op.parse_args()
    if args:
        root_dir = args[:]
    else:
        root_dir = ["."]

    try:
        for paths in find_duplicates(root_dir):
            print("Duplicates:")
            for path in paths:
                print("    ", path)
            # do something with duplicates here.
    except KeyboardInterrupt:
        status()
        print("Interrupted.")

    if opts.verbose:
        print("%d files compared by header" % len(file_headers))
        print("%d files compared by hash" % len(file_hashes))
